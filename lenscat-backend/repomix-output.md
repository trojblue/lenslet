This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-09-09 21:50:15

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
src
  lenscat_backend
    __init__.py
    utils
      thumbnails.py
      __init__.py
      hashing.py
      exif.py
    main.py
    models
      __init__.py
      types.py
    workers
      __init__.py
      indexer.py
      thumbnailer.py
    api
      thumbnails.py
      __init__.py
      health.py
      items.py
      folders.py
      search.py
      dependencies.py
    storage
      base.py
      __init__.py
      s3.py
      local.py
pyproject.toml
data
  events
    2024
      conference
        keynote.jpg.json
        panel.jpg.json
  portraits
    person1.jpg.json
    person2.jpg.json
  landscapes
    sunset.jpg.json
    mountain.jpg.json
    lake.jpg.json
  architecture
    bridge.jpg.json
    building1.jpg.json
requirements.txt
env.example
README.md
scripts
  dev.py
  create_sample_data.py
```

# Repository Files


## src/lenscat_backend/__init__.py

- Characters: 232
- Tokens: 0

```python
import uvicorn
from .main import app


def main() -> None:
    """Run the FastAPI application server."""
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=True,
        reload_dirs=["src"]
    )
```

## src/lenscat_backend/utils/thumbnails.py

- Characters: 4638
- Tokens: 0

```python
"""Thumbnail generation utilities."""
import asyncio
import io
from typing import Optional

try:
    import pyvips
    PYVIPS_AVAILABLE = True
except ImportError:
    PYVIPS_AVAILABLE = False
    pyvips = None

from PIL import Image

from ..storage.base import StorageBackend


class ThumbnailGenerator:
    """Handles thumbnail generation with pyvips fallback to PIL."""

    def __init__(self, max_size: int = 256, quality: int = 70):
        """Initialize thumbnail generator."""
        self.max_size = max_size
        self.quality = quality

    async def generate_thumbnail(
        self, 
        storage: StorageBackend, 
        image_path: str
    ) -> Optional[bytes]:
        """Generate thumbnail for an image."""
        try:
            # Read source image
            image_data = await storage.read_bytes(image_path)
            
            # Generate thumbnail using pyvips if available, otherwise PIL
            if PYVIPS_AVAILABLE:
                thumbnail_data = await self._generate_with_pyvips(image_data)
            else:
                thumbnail_data = await self._generate_with_pil(image_data)
            
            return thumbnail_data
            
        except Exception as e:
            print(f"Failed to generate thumbnail for {image_path}: {e}")
            return None

    async def _generate_with_pyvips(self, image_data: bytes) -> bytes:
        """Generate thumbnail using pyvips (faster)."""
        def _process():
            # Load image from memory
            image = pyvips.Image.new_from_buffer(image_data, "")
            
            # Calculate resize dimensions
            width, height = image.width, image.height
            if width > height:
                new_width = self.max_size
                new_height = int((height * self.max_size) / width)
            else:
                new_height = self.max_size
                new_width = int((width * self.max_size) / height)
            
            # Resize image
            resized = image.resize(new_width / width)
            
            # Convert to WebP
            return resized.webpsave_buffer(Q=self.quality)
        
        # Run in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, _process)

    async def _generate_with_pil(self, image_data: bytes) -> bytes:
        """Generate thumbnail using PIL (fallback)."""
        def _process():
            # Load image
            with Image.open(io.BytesIO(image_data)) as image:
                # Convert to RGB if necessary
                if image.mode in ("RGBA", "LA", "P"):
                    rgb_image = Image.new("RGB", image.size, (255, 255, 255))
                    if image.mode == "P":
                        image = image.convert("RGBA")
                    rgb_image.paste(image, mask=image.split()[-1] if image.mode in ("RGBA", "LA") else None)
                    image = rgb_image
                
                # Calculate resize dimensions
                width, height = image.size
                if width > height:
                    new_width = self.max_size
                    new_height = int((height * self.max_size) / width)
                else:
                    new_height = self.max_size
                    new_width = int((width * self.max_size) / height)
                
                # Resize with high quality
                thumbnail = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
                
                # Save as WebP
                output = io.BytesIO()
                thumbnail.save(output, format="WEBP", quality=self.quality, optimize=True)
                return output.getvalue()
        
        # Run in thread pool
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, _process)

    async def ensure_thumbnail(
        self, 
        storage: StorageBackend, 
        image_path: str
    ) -> bool:
        """Ensure thumbnail exists, generate if missing."""
        thumb_path = f"{image_path}.thumbnail"
        
        # Check if thumbnail already exists
        if await storage.exists(thumb_path):
            return True
        
        # Generate thumbnail
        thumbnail_data = await self.generate_thumbnail(storage, image_path)
        if thumbnail_data is None:
            return False
        
        # Save thumbnail
        try:
            await storage.write_bytes(thumb_path, thumbnail_data)
            return True
        except Exception as e:
            print(f"Failed to save thumbnail for {image_path}: {e}")
            return False
```

## src/lenscat_backend/utils/__init__.py

- Characters: 274
- Tokens: 0

```python
"""Utilities package."""
from .exif import extract_exif
from .hashing import compute_file_hash, compute_hash_chunked
from .thumbnails import ThumbnailGenerator

__all__ = [
    "extract_exif",
    "compute_file_hash", 
    "compute_hash_chunked",
    "ThumbnailGenerator",
]
```

## src/lenscat_backend/utils/hashing.py

- Characters: 1290
- Tokens: 0

```python
"""File hashing utilities."""
import asyncio
import hashlib
from typing import Optional

try:
    import blake3
    BLAKE3_AVAILABLE = True
except ImportError:
    BLAKE3_AVAILABLE = False
    blake3 = None

from ..storage.base import StorageBackend


async def compute_file_hash(storage: StorageBackend, path: str) -> Optional[str]:
    """Compute BLAKE3 hash of a file, fallback to SHA256."""
    try:
        data = await storage.read_bytes(path)
        
        if BLAKE3_AVAILABLE:
            # Use BLAKE3 (faster)
            hash_obj = blake3.blake3()
            hash_obj.update(data)
            return f"blake3:{hash_obj.hexdigest()}"
        else:
            # Fallback to SHA256
            hash_obj = hashlib.sha256()
            hash_obj.update(data)
            return f"sha256:{hash_obj.hexdigest()}"
            
    except Exception as e:
        print(f"Failed to compute hash for {path}: {e}")
        return None


async def compute_hash_chunked(storage: StorageBackend, path: str, chunk_size: int = 8192) -> Optional[str]:
    """Compute hash for large files in chunks (for future streaming support)."""
    # For now, just use the simple method since our storage interface
    # doesn't support streaming reads yet
    return await compute_file_hash(storage, path)
```

## src/lenscat_backend/utils/exif.py

- Characters: 1942
- Tokens: 0

```python
"""EXIF data extraction utilities."""
import asyncio
import io
from datetime import datetime
from typing import Optional

from PIL import Image
from PIL.ExifTags import TAGS

from ..models.types import ExifData
from ..storage.base import StorageBackend


async def extract_exif(storage: StorageBackend, image_path: str) -> Optional[ExifData]:
    """Extract basic EXIF data from an image."""
    try:
        # Read image data
        image_data = await storage.read_bytes(image_path)
        
        # Process in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, _extract_exif_sync, image_data)
        
    except Exception as e:
        print(f"Failed to extract EXIF from {image_path}: {e}")
        return None


def _extract_exif_sync(image_data: bytes) -> Optional[ExifData]:
    """Synchronous EXIF extraction."""
    try:
        with Image.open(io.BytesIO(image_data)) as image:
            # Get basic dimensions
            width, height = image.size
            created_at = None
            
            # Extract EXIF data if available
            exif_dict = image.getexif()
            if exif_dict:
                # Look for DateTime tags
                for tag_id, value in exif_dict.items():
                    tag = TAGS.get(tag_id, tag_id)
                    
                    if tag in ("DateTime", "DateTimeOriginal", "DateTimeDigitized"):
                        try:
                            # Parse datetime string
                            created_at = datetime.strptime(value, "%Y:%m:%d %H:%M:%S")
                            break
                        except (ValueError, TypeError):
                            continue
            
            return ExifData(
                width=width,
                height=height,
                createdAt=created_at
            )
            
    except Exception:
        return None
```

## src/lenscat_backend/main.py

- Characters: 1980
- Tokens: 0

```python
"""Main FastAPI application."""
import os
from contextlib import asynccontextmanager

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles

from .api import api_router


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    # Startup
    print("ðŸš€ Lenscat backend starting up...")
    
    # TODO: Initialize background workers here if needed
    
    yield
    
    # Shutdown
    print("ðŸ›‘ Lenscat backend shutting down...")


def create_app() -> FastAPI:
    """Create and configure FastAPI application."""
    app = FastAPI(
        title="Lenscat Backend",
        description="Minimal gallery backend with flat file storage",
        version="0.1.0",
        lifespan=lifespan
    )
    
    # CORS middleware for frontend
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # In production, specify actual origins
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Include API routes
    app.include_router(api_router)
    
    # Health check at root
    @app.get("/")
    async def root():
        return {"message": "Lenscat Backend", "version": "0.1.0"}
    
    # Serve static files for local storage (development)
    if os.getenv("STORAGE_TYPE", "local").lower() == "local":
        static_dir = os.getenv("LOCAL_ROOT", "./data")
        if os.path.exists(static_dir):
            app.mount("/api/files", StaticFiles(directory=static_dir), name="files")
    
    return app


# Create app instance
app = create_app()


if __name__ == "__main__":
    import uvicorn
    
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    reload = os.getenv("RELOAD", "false").lower() == "true"
    
    uvicorn.run(
        "src.main:app",
        host=host,
        port=port,
        reload=reload,
        reload_dirs=["src"] if reload else None
    )
```

## src/lenscat_backend/models/__init__.py

- Characters: 537
- Tokens: 0

```python
"""Data models package."""
from .types import (
    DirEntry,
    DirKind,
    ExifData,
    FolderIndex,
    HealthStatus,
    Item,
    MimeType,
    PointerConfig,
    PointerTarget,
    RollupItem,
    RollupManifest,
    SearchResult,
    Sidecar,
    StorageType,
)

__all__ = [
    "DirEntry",
    "DirKind", 
    "ExifData",
    "FolderIndex",
    "HealthStatus",
    "Item",
    "MimeType",
    "PointerConfig",
    "PointerTarget",
    "RollupItem",
    "RollupManifest",
    "SearchResult",
    "Sidecar",
    "StorageType",
]
```

## src/lenscat_backend/models/types.py

- Characters: 3338
- Tokens: 0

```python
"""Core data models matching frontend types."""
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Union

from pydantic import BaseModel, Field


class MimeType(str, Enum):
    """Supported image MIME types."""
    WEBP = "image/webp"
    JPEG = "image/jpeg"
    PNG = "image/png"


class DirKind(str, Enum):
    """Directory types."""
    BRANCH = "branch"
    LEAF_REAL = "leaf-real"
    LEAF_POINTER = "leaf-pointer"


class StorageType(str, Enum):
    """Storage backend types."""
    LOCAL = "local"
    S3 = "s3"


class Item(BaseModel):
    """Image item metadata."""
    path: str
    name: str
    type: MimeType
    w: int
    h: int
    size: int
    hasThumb: bool = Field(alias="has_thumb")
    hasMeta: bool = Field(alias="has_meta")
    hash: Optional[str] = None

    class Config:
        allow_population_by_field_name = True


class DirEntry(BaseModel):
    """Directory entry."""
    name: str
    kind: DirKind


class ExifData(BaseModel):
    """Basic EXIF metadata."""
    width: Optional[int] = None
    height: Optional[int] = None
    createdAt: Optional[datetime] = Field(alias="created_at", default=None)

    class Config:
        allow_population_by_field_name = True


class Sidecar(BaseModel):
    """Sidecar metadata file."""
    v: int = 1
    tags: List[str] = Field(default_factory=list)
    notes: str = ""
    exif: Optional[ExifData] = None
    hash: Optional[str] = None
    updatedAt: datetime = Field(alias="updated_at")
    updatedBy: str = Field(alias="updated_by")

    class Config:
        allow_population_by_field_name = True


class FolderIndex(BaseModel):
    """Folder manifest (_index.json)."""
    v: int = 1
    path: str
    generatedAt: datetime = Field(alias="generated_at")
    items: List[Item] = Field(default_factory=list)
    dirs: List[DirEntry] = Field(default_factory=list)
    page: Optional[int] = None
    pageCount: Optional[int] = Field(alias="page_count", default=None)

    class Config:
        allow_population_by_field_name = True


class RollupItem(BaseModel):
    """Flattened item for search."""
    path: str
    name: str
    tags: List[str] = Field(default_factory=list)
    notes: str = ""


class RollupManifest(BaseModel):
    """Search rollup manifest (_rollup.json)."""
    v: int = 1
    generatedAt: datetime = Field(alias="generated_at")
    items: List[RollupItem] = Field(default_factory=list)

    class Config:
        allow_population_by_field_name = True


class PointerTarget(BaseModel):
    """Pointer folder target configuration."""
    type: StorageType
    bucket: Optional[str] = None
    prefix: Optional[str] = None
    region: Optional[str] = None
    path: Optional[str] = None


class PointerConfig(BaseModel):
    """Pointer folder configuration (.lenscat.folder.json)."""
    version: int = 1
    kind: str = "pointer"
    target: PointerTarget
    label: Optional[str] = None
    readonly: bool = False


class SearchResult(BaseModel):
    """Search API response."""
    items: List[RollupItem]
    total: int = 0


class HealthStatus(BaseModel):
    """Health check response."""
    status: str = "healthy"
    backend: Dict[str, Any] = Field(default_factory=dict)
    workers: Dict[str, Any] = Field(default_factory=dict)
    storage: Dict[str, Any] = Field(default_factory=dict)
```

## src/lenscat_backend/workers/__init__.py

- Characters: 178
- Tokens: 0

```python
"""Workers package."""
from .indexer import FolderIndexer, RollupBuilder
from .thumbnailer import ThumbnailWorker

__all__ = ["FolderIndexer", "RollupBuilder", "ThumbnailWorker"]
```

## src/lenscat_backend/workers/indexer.py

- Characters: 6055
- Tokens: 0

```python
"""Indexing worker for building manifests."""
import asyncio
from datetime import datetime
from typing import List, Optional

import orjson

from ..models.types import FolderIndex, RollupItem, RollupManifest, Sidecar
from ..storage.base import StorageBackend
from ..utils import compute_file_hash, extract_exif


class FolderIndexer:
    """Builds and maintains folder index manifests."""

    def __init__(self, storage: StorageBackend):
        """Initialize indexer with storage backend."""
        self.storage = storage

    async def build_folder_index(self, folder_path: str) -> FolderIndex:
        """Build _index.json for a folder."""
        dirs, items = await self.storage.list_directory(folder_path)
        
        # Enrich items with hashes and EXIF if missing
        enriched_items = []
        for item in items:
            # Check if we have a sidecar with hash
            sidecar_path = f"{item.path}.json"
            hash_value = item.hash
            
            if await self.storage.exists(sidecar_path):
                try:
                    sidecar_data = await self.storage.read_text(sidecar_path)
                    sidecar = Sidecar.parse_raw(sidecar_data)
                    if sidecar.hash:
                        hash_value = sidecar.hash
                except Exception:
                    pass
            
            # Compute hash if missing
            if not hash_value:
                hash_value = await compute_file_hash(self.storage, item.path)
            
            # Update item
            enriched_items.append(item.copy(update={"hash": hash_value}))
        
        return FolderIndex(
            path=folder_path,
            generatedAt=datetime.utcnow(),
            items=enriched_items,
            dirs=dirs
        )

    async def save_folder_index(self, folder_path: str, index: FolderIndex) -> None:
        """Save folder index to _index.json."""
        index_path = f"{folder_path.rstrip('/')}/_index.json"
        
        # Serialize with orjson for performance
        data = orjson.dumps(
            index.dict(by_alias=True), 
            option=orjson.OPT_UTC_Z | orjson.OPT_INDENT_2
        )
        
        await self.storage.write_bytes(index_path, data)

    async def build_and_save_index(self, folder_path: str) -> FolderIndex:
        """Build and save folder index in one operation."""
        index = await self.build_folder_index(folder_path)
        await self.save_folder_index(folder_path, index)
        return index

    async def is_index_stale(self, folder_path: str) -> bool:
        """Check if folder index needs rebuilding."""
        index_path = f"{folder_path.rstrip('/')}/_index.json"
        
        if not await self.storage.exists(index_path):
            return True
        
        # For simplicity in MVP, consider index stale if older than 1 hour
        # In production, you'd compare with actual file mtimes
        try:
            index_data = await self.storage.read_text(index_path)
            index = FolderIndex.parse_raw(index_data)
            
            # Check if index is older than 1 hour
            age = datetime.utcnow() - index.generatedAt
            return age.total_seconds() > 3600
            
        except Exception:
            return True


class RollupBuilder:
    """Builds search rollup manifests."""

    def __init__(self, storage: StorageBackend):
        """Initialize rollup builder."""
        self.storage = storage

    async def build_rollup_manifest(self, root_path: str = "") -> RollupManifest:
        """Build _rollup.json for search across all folders."""
        items = []
        
        # Recursively collect all items with metadata
        await self._collect_items_recursive(root_path, items)
        
        return RollupManifest(
            generatedAt=datetime.utcnow(),
            items=items
        )

    async def _collect_items_recursive(self, folder_path: str, items: List[RollupItem]) -> None:
        """Recursively collect items from all folders."""
        try:
            dirs, folder_items = await self.storage.list_directory(folder_path)
            
            # Process items in current folder
            for item in folder_items:
                # Load sidecar if available
                sidecar_path = f"{item.path}.json"
                tags = []
                notes = ""
                
                if await self.storage.exists(sidecar_path):
                    try:
                        sidecar_data = await self.storage.read_text(sidecar_path)
                        sidecar = Sidecar.parse_raw(sidecar_data)
                        tags = sidecar.tags
                        notes = sidecar.notes
                    except Exception:
                        pass
                
                items.append(RollupItem(
                    path=item.path,
                    name=item.name,
                    tags=tags,
                    notes=notes
                ))
            
            # Recurse into subdirectories
            for dir_entry in dirs:
                subdir_path = f"{folder_path.rstrip('/')}/{dir_entry.name}"
                await self._collect_items_recursive(subdir_path, items)
                
        except Exception as e:
            print(f"Error collecting items from {folder_path}: {e}")

    async def save_rollup_manifest(self, root_path: str, manifest: RollupManifest) -> None:
        """Save rollup manifest to _rollup.json."""
        rollup_path = f"{root_path.rstrip('/')}/_rollup.json"
        
        data = orjson.dumps(
            manifest.dict(by_alias=True),
            option=orjson.OPT_UTC_Z | orjson.OPT_INDENT_2
        )
        
        await self.storage.write_bytes(rollup_path, data)

    async def build_and_save_rollup(self, root_path: str = "") -> RollupManifest:
        """Build and save rollup manifest."""
        manifest = await self.build_rollup_manifest(root_path)
        await self.save_rollup_manifest(root_path, manifest)
        return manifest
```

## src/lenscat_backend/workers/thumbnailer.py

- Characters: 3521
- Tokens: 0

```python
"""Thumbnail generation worker."""
import asyncio
from typing import List

from ..storage.base import StorageBackend
from ..utils.thumbnails import ThumbnailGenerator


class ThumbnailWorker:
    """Worker for generating missing thumbnails."""

    def __init__(self, storage: StorageBackend, max_concurrent: int = 4):
        """Initialize thumbnail worker."""
        self.storage = storage
        self.generator = ThumbnailGenerator()
        self.max_concurrent = max_concurrent
        self._semaphore = asyncio.Semaphore(max_concurrent)

    async def generate_missing_thumbnails(self, folder_path: str) -> int:
        """Generate thumbnails for all images missing them in a folder."""
        _, items = await self.storage.list_directory(folder_path)
        
        # Find items without thumbnails
        missing_thumbs = [item for item in items if not item.hasThumb]
        
        if not missing_thumbs:
            return 0
        
        # Generate thumbnails concurrently
        tasks = [
            self._generate_thumbnail_safe(item.path)
            for item in missing_thumbs
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Count successful generations
        success_count = sum(1 for result in results if result is True)
        
        print(f"Generated {success_count}/{len(missing_thumbs)} thumbnails for {folder_path}")
        return success_count

    async def _generate_thumbnail_safe(self, image_path: str) -> bool:
        """Generate thumbnail with concurrency control."""
        async with self._semaphore:
            try:
                return await self.generator.ensure_thumbnail(self.storage, image_path)
            except Exception as e:
                print(f"Failed to generate thumbnail for {image_path}: {e}")
                return False

    async def process_folder_recursive(self, root_path: str) -> int:
        """Process all folders recursively to generate missing thumbnails."""
        total_generated = 0
        
        try:
            # Process current folder
            generated = await self.generate_missing_thumbnails(root_path)
            total_generated += generated
            
            # Get subdirectories
            dirs, _ = await self.storage.list_directory(root_path)
            
            # Process subdirectories
            for dir_entry in dirs:
                subdir_path = f"{root_path.rstrip('/')}/{dir_entry.name}"
                subdir_generated = await self.process_folder_recursive(subdir_path)
                total_generated += subdir_generated
                
        except Exception as e:
            print(f"Error processing folder {root_path}: {e}")
        
        return total_generated

    async def get_thumbnail_stats(self, folder_path: str) -> dict:
        """Get thumbnail statistics for a folder."""
        try:
            _, items = await self.storage.list_directory(folder_path)
            
            total_images = len(items)
            with_thumbs = sum(1 for item in items if item.hasThumb)
            missing_thumbs = total_images - with_thumbs
            
            return {
                "total_images": total_images,
                "with_thumbnails": with_thumbs,
                "missing_thumbnails": missing_thumbs,
                "coverage_percent": (with_thumbs / total_images * 100) if total_images > 0 else 100
            }
            
        except Exception as e:
            return {"error": str(e)}
```

## src/lenscat_backend/api/thumbnails.py

- Characters: 4446
- Tokens: 0

```python
"""Thumbnail API endpoints."""
from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import Response
from typing import Optional

from ..storage.base import StorageBackend
from ..utils.thumbnails import ThumbnailGenerator
from .dependencies import get_storage_dependency as get_storage

router = APIRouter(prefix="/thumb", tags=["thumbnails"])


@router.get("")
async def get_thumbnail(
    path: str = Query(..., description="Image path"),
    storage: StorageBackend = Depends(get_storage)
):
    """Get thumbnail for an image, generate if missing."""
    try:
        # Check if image exists
        if not await storage.exists(path):
            raise HTTPException(status_code=404, detail="Image not found")
        
        thumbnail_path = f"{path}.thumbnail"
        
        # Return existing thumbnail if available
        if await storage.exists(thumbnail_path):
            try:
                thumbnail_data = await storage.read_bytes(thumbnail_path)
                return Response(
                    content=thumbnail_data,
                    media_type="image/webp",
                    headers={
                        "Cache-Control": "public, max-age=86400",  # Cache for 1 day
                        "ETag": f'"{hash(thumbnail_data)}"'
                    }
                )
            except Exception as e:
                print(f"Failed to read existing thumbnail for {path}: {e}")
                # Fall through to regenerate
        
        # Generate thumbnail on demand
        generator = ThumbnailGenerator()
        thumbnail_data = await generator.generate_thumbnail(storage, path)
        
        if thumbnail_data is None:
            raise HTTPException(status_code=500, detail="Failed to generate thumbnail")
        
        # Save thumbnail for future use
        try:
            await storage.write_bytes(thumbnail_path, thumbnail_data)
        except Exception as e:
            print(f"Failed to save thumbnail for {path}: {e}")
            # Continue anyway, we can still return the generated thumbnail
        
        return Response(
            content=thumbnail_data,
            media_type="image/webp",
            headers={
                "Cache-Control": "public, max-age=86400",
                "ETag": f'"{hash(thumbnail_data)}"'
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get thumbnail: {str(e)}")


@router.post("")
async def generate_thumbnail(
    path: str = Query(..., description="Image path"),
    force: bool = Query(False, description="Force regeneration even if exists"),
    storage: StorageBackend = Depends(get_storage)
):
    """Generate thumbnail for an image."""
    try:
        # Check if image exists
        if not await storage.exists(path):
            raise HTTPException(status_code=404, detail="Image not found")
        
        thumbnail_path = f"{path}.thumbnail"
        
        # Check if thumbnail already exists and force is not set
        if not force and await storage.exists(thumbnail_path):
            return {"message": "Thumbnail already exists", "path": thumbnail_path}
        
        # Generate thumbnail
        generator = ThumbnailGenerator()
        success = await generator.ensure_thumbnail(storage, path)
        
        if success:
            return {"message": "Thumbnail generated successfully", "path": thumbnail_path}
        else:
            raise HTTPException(status_code=500, detail="Failed to generate thumbnail")
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to generate thumbnail: {str(e)}")


@router.delete("")
async def delete_thumbnail(
    path: str = Query(..., description="Image path"),
    storage: StorageBackend = Depends(get_storage)
):
    """Delete thumbnail for an image."""
    try:
        thumbnail_path = f"{path}.thumbnail"
        
        if await storage.exists(thumbnail_path):
            await storage.delete(thumbnail_path)
            return {"message": "Thumbnail deleted"}
        else:
            raise HTTPException(status_code=404, detail="Thumbnail not found")
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete thumbnail: {str(e)}")
```

## src/lenscat_backend/api/__init__.py

- Characters: 434
- Tokens: 0

```python
"""API package."""
from fastapi import APIRouter

from . import folders, health, items, search, thumbnails

# Create main API router
api_router = APIRouter(prefix="/api")

# Include all route modules
api_router.include_router(folders.router)
api_router.include_router(items.router)
api_router.include_router(thumbnails.router)
api_router.include_router(search.router)
api_router.include_router(health.router)

__all__ = ["api_router"]
```

## src/lenscat_backend/api/health.py

- Characters: 2716
- Tokens: 0

```python
"""Health check API endpoints."""
from fastapi import APIRouter, Depends
from datetime import datetime

from ..models.types import HealthStatus
from ..storage.base import StorageBackend
from ..workers.thumbnailer import ThumbnailWorker
from .dependencies import get_storage_dependency as get_storage

router = APIRouter(prefix="/health", tags=["health"])


@router.get("", response_model=HealthStatus)
async def health_check(
    storage: StorageBackend = Depends(get_storage)
):
    """Get overall system health status."""
    try:
        # Check storage backend health
        storage_health = await storage.health_check()
        
        # Get thumbnail statistics
        thumb_worker = ThumbnailWorker(storage)
        thumb_stats = await thumb_worker.get_thumbnail_stats("")
        
        # Check for key system files
        has_rollup = await storage.exists("_rollup.json")
        has_root_index = await storage.exists("_index.json")
        
        return HealthStatus(
            status="healthy",
            backend={
                "timestamp": datetime.utcnow().isoformat(),
                "version": "0.1.0",
                "uptime_seconds": 0,  # TODO: Track actual uptime
            },
            workers={
                "thumbnail_stats": thumb_stats,
                "indexer_status": "operational",
            },
            storage={
                **storage_health,
                "has_rollup_manifest": has_rollup,
                "has_root_index": has_root_index,
            }
        )
        
    except Exception as e:
        return HealthStatus(
            status="unhealthy",
            backend={
                "timestamp": datetime.utcnow().isoformat(),
                "error": str(e),
            }
        )


@router.get("/storage")
async def storage_health(
    storage: StorageBackend = Depends(get_storage)
):
    """Get detailed storage backend health."""
    return await storage.health_check()


@router.get("/workers")
async def worker_health(
    storage: StorageBackend = Depends(get_storage)
):
    """Get worker system health and statistics."""
    try:
        thumb_worker = ThumbnailWorker(storage)
        
        # Get stats for root directory
        root_stats = await thumb_worker.get_thumbnail_stats("")
        
        return {
            "thumbnail_worker": {
                "status": "operational",
                "stats": root_stats,
            },
            "indexer": {
                "status": "operational",
                "last_run": None,  # TODO: Track last run times
            }
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "status": "unhealthy"
        }
```

## src/lenscat_backend/api/items.py

- Characters: 4225
- Tokens: 0

```python
"""Item API endpoints."""
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import Response

import orjson

from ..models.types import Sidecar
from ..storage.base import StorageBackend
from ..utils import extract_exif
from .dependencies import get_storage_dependency as get_storage

router = APIRouter(prefix="/item", tags=["items"])


@router.get("", response_model=Sidecar)
async def get_item(
    path: str = Query(..., description="Item path"),
    storage: StorageBackend = Depends(get_storage)
):
    """Get item metadata, merging sidecar with EXIF."""
    try:
        # Check if image exists
        if not await storage.exists(path):
            raise HTTPException(status_code=404, detail="Image not found")
        
        sidecar_path = f"{path}.json"
        sidecar = None
        
        # Load existing sidecar if it exists
        if await storage.exists(sidecar_path):
            try:
                sidecar_data = await storage.read_text(sidecar_path)
                sidecar = Sidecar.parse_raw(sidecar_data)
            except Exception as e:
                print(f"Failed to parse sidecar for {path}: {e}")
        
        # If no sidecar exists, create basic one with EXIF
        if sidecar is None:
            exif_data = await extract_exif(storage, path)
            sidecar = Sidecar(
                tags=[],
                notes="",
                exif=exif_data,
                updatedAt=datetime.utcnow(),
                updatedBy="system"
            )
        
        return sidecar
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get item: {str(e)}")


@router.put("", response_model=Sidecar)
async def update_item(
    path: str = Query(..., description="Item path"),
    sidecar: Sidecar,
    storage: StorageBackend = Depends(get_storage)
):
    """Update item metadata."""
    try:
        # Check if image exists
        if not await storage.exists(path):
            raise HTTPException(status_code=404, detail="Image not found")
        
        sidecar_path = f"{path}.json"
        
        # Handle conflict resolution (last-writer-wins with additive tags)
        existing_sidecar = None
        if await storage.exists(sidecar_path):
            try:
                existing_data = await storage.read_text(sidecar_path)
                existing_sidecar = Sidecar.parse_raw(existing_data)
            except Exception:
                pass
        
        # Merge tags additively if there's an existing sidecar
        if existing_sidecar and existing_sidecar.updatedAt > sidecar.updatedAt:
            # Existing is newer, merge tags additively
            merged_tags = list(set(existing_sidecar.tags + sidecar.tags))
            sidecar.tags = merged_tags
            sidecar.updatedAt = datetime.utcnow()
        
        # Ensure EXIF is preserved if not provided
        if not sidecar.exif and existing_sidecar and existing_sidecar.exif:
            sidecar.exif = existing_sidecar.exif
        
        # Save updated sidecar
        sidecar_json = orjson.dumps(
            sidecar.dict(by_alias=True),
            option=orjson.OPT_UTC_Z | orjson.OPT_INDENT_2
        )
        
        await storage.write_bytes(sidecar_path, sidecar_json)
        
        return sidecar
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to update item: {str(e)}")


@router.delete("")
async def delete_item_metadata(
    path: str = Query(..., description="Item path"),
    storage: StorageBackend = Depends(get_storage)
):
    """Delete item sidecar metadata."""
    try:
        sidecar_path = f"{path}.json"
        
        if await storage.exists(sidecar_path):
            await storage.delete(sidecar_path)
            return {"message": "Metadata deleted"}
        else:
            raise HTTPException(status_code=404, detail="No metadata found")
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete metadata: {str(e)}")
```

## src/lenscat_backend/api/folders.py

- Characters: 3457
- Tokens: 0

```python
"""Folder API endpoints."""
from fastapi import APIRouter, Depends, HTTPException, Query
from typing import Optional

from ..models.types import FolderIndex
from ..storage.base import StorageBackend
from ..workers.indexer import FolderIndexer
from .dependencies import get_storage_dependency as get_storage

router = APIRouter(prefix="/folders", tags=["folders"])


@router.get("", response_model=FolderIndex)
async def get_folder(
    path: str = Query(..., description="Folder path to list"),
    page: Optional[int] = Query(None, description="Page number for pagination"),
    storage: StorageBackend = Depends(get_storage)
):
    """Get folder contents, building index if missing or stale."""
    try:
        indexer = FolderIndexer(storage)
        
        # Clean up path
        folder_path = path.strip("/")
        if not folder_path:
            folder_path = ""
        
        # Check if we have a valid index
        index_path = f"{folder_path}/_index.json" if folder_path else "_index.json"
        
        if await storage.exists(index_path) and not await indexer.is_index_stale(folder_path):
            # Load existing index
            try:
                index_data = await storage.read_text(index_path)
                index = FolderIndex.parse_raw(index_data)
                
                # Handle pagination if requested
                if page is not None:
                    # Simple pagination (could be optimized)
                    page_size = 100
                    start_idx = page * page_size
                    end_idx = start_idx + page_size
                    
                    total_items = len(index.items)
                    page_count = (total_items + page_size - 1) // page_size
                    
                    index.items = index.items[start_idx:end_idx]
                    index.page = page
                    index.pageCount = page_count
                
                return index
                
            except Exception as e:
                print(f"Failed to load existing index: {e}")
                # Fall through to rebuild
        
        # Build new index
        index = await indexer.build_and_save_index(folder_path)
        
        # Handle pagination
        if page is not None:
            page_size = 100
            start_idx = page * page_size
            end_idx = start_idx + page_size
            
            total_items = len(index.items)
            page_count = (total_items + page_size - 1) // page_size
            
            index.items = index.items[start_idx:end_idx]
            index.page = page
            index.pageCount = page_count
        
        return index
        
    except Exception as e:
        print(f"Error getting folder {path}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get folder: {str(e)}")


@router.post("/{folder_path:path}/reindex")
async def reindex_folder(
    folder_path: str,
    storage: StorageBackend = Depends(get_storage)
):
    """Force reindex of a folder."""
    try:
        indexer = FolderIndexer(storage)
        index = await indexer.build_and_save_index(folder_path)
        
        return {
            "message": f"Reindexed folder: {folder_path}",
            "items_count": len(index.items),
            "dirs_count": len(index.dirs)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to reindex: {str(e)}")
```

## src/lenscat_backend/api/search.py

- Characters: 4467
- Tokens: 0

```python
"""Search API endpoints."""
from fastapi import APIRouter, Depends, HTTPException, Query
from typing import List

from ..models.types import RollupManifest, SearchResult, RollupItem
from ..storage.base import StorageBackend
from ..workers.indexer import RollupBuilder
from .dependencies import get_storage_dependency as get_storage

router = APIRouter(prefix="/search", tags=["search"])


@router.get("", response_model=SearchResult)
async def search_items(
    q: str = Query(..., description="Search query"),
    limit: int = Query(100, description="Maximum results to return"),
    storage: StorageBackend = Depends(get_storage)
):
    """Search for items by filename, tags, and notes."""
    try:
        if not q.strip():
            return SearchResult(items=[], total=0)
        
        # Load rollup manifest
        rollup_path = "_rollup.json"
        rollup_manifest = None
        
        if await storage.exists(rollup_path):
            try:
                rollup_data = await storage.read_text(rollup_path)
                rollup_manifest = RollupManifest.parse_raw(rollup_data)
            except Exception as e:
                print(f"Failed to load rollup manifest: {e}")
        
        # If no rollup exists, build one on-demand (for development)
        if rollup_manifest is None:
            print("No rollup manifest found, building on-demand...")
            builder = RollupBuilder(storage)
            rollup_manifest = await builder.build_and_save_rollup("")
        
        # Perform simple text search
        query_terms = q.lower().split()
        matching_items = []
        
        for item in rollup_manifest.items:
            # Create searchable text
            searchable_text = " ".join([
                item.name.lower(),
                " ".join(item.tags).lower(),
                item.notes.lower()
            ])
            
            # Check if all query terms are present
            if all(term in searchable_text for term in query_terms):
                matching_items.append(item)
        
        # Apply limit
        limited_items = matching_items[:limit]
        
        return SearchResult(
            items=limited_items,
            total=len(matching_items)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")


@router.post("/rebuild-index")
async def rebuild_search_index(
    storage: StorageBackend = Depends(get_storage)
):
    """Rebuild the search rollup index."""
    try:
        builder = RollupBuilder(storage)
        manifest = await builder.build_and_save_rollup("")
        
        return {
            "message": "Search index rebuilt",
            "items_indexed": len(manifest.items),
            "generated_at": manifest.generatedAt.isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to rebuild index: {str(e)}")


@router.get("/suggest")
async def get_search_suggestions(
    q: str = Query("", description="Partial search query"),
    limit: int = Query(10, description="Maximum suggestions"),
    storage: StorageBackend = Depends(get_storage)
):
    """Get search suggestions based on existing tags and filenames."""
    try:
        # Load rollup manifest
        rollup_path = "_rollup.json"
        
        if not await storage.exists(rollup_path):
            return {"suggestions": []}
        
        rollup_data = await storage.read_text(rollup_path)
        rollup_manifest = RollupManifest.parse_raw(rollup_data)
        
        # Collect all unique tags and common filename parts
        suggestions = set()
        query_lower = q.lower()
        
        for item in rollup_manifest.items:
            # Add matching tags
            for tag in item.tags:
                if query_lower in tag.lower():
                    suggestions.add(tag)
            
            # Add matching filename parts (without extension)
            name_parts = item.name.lower().replace(".", " ").split()
            for part in name_parts:
                if len(part) > 2 and query_lower in part:
                    suggestions.add(part)
        
        # Sort and limit suggestions
        sorted_suggestions = sorted(suggestions)[:limit]
        
        return {"suggestions": sorted_suggestions}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get suggestions: {str(e)}")
```

## src/lenscat_backend/api/dependencies.py

- Characters: 1512
- Tokens: 0

```python
"""FastAPI dependencies."""
import os
from functools import lru_cache

from ..storage.base import StorageBackend
from ..storage.local import LocalStorage
from ..storage.s3 import S3Storage


@lru_cache()
def get_storage() -> StorageBackend:
    """Get storage backend based on environment configuration."""
    storage_type = os.getenv("STORAGE_TYPE", "local").lower()
    
    if storage_type == "s3":
        bucket = os.getenv("S3_BUCKET")
        if not bucket:
            raise ValueError("S3_BUCKET environment variable is required for S3 storage")
        
        prefix = os.getenv("S3_PREFIX", "")
        region = os.getenv("S3_REGION", "us-east-1")
        
        return S3Storage(bucket=bucket, prefix=prefix, region=region)
    
    elif storage_type == "local":
        root_path = os.getenv("LOCAL_ROOT", "./data")
        return LocalStorage(root_path=root_path)
    
    else:
        raise ValueError(f"Unsupported storage type: {storage_type}")


# For testing/development, allow override
_storage_override = None

def override_storage(storage: StorageBackend) -> None:
    """Override storage for testing."""
    global _storage_override
    _storage_override = storage

def clear_storage_override() -> None:
    """Clear storage override."""
    global _storage_override
    _storage_override = None

def get_storage_dependency() -> StorageBackend:
    """Dependency function for FastAPI."""
    if _storage_override is not None:
        return _storage_override
    return get_storage()
```

## src/lenscat_backend/storage/base.py

- Characters: 1594
- Tokens: 0

```python
"""Abstract storage interface."""
from abc import ABC, abstractmethod
from typing import AsyncIterator, List, Optional, Tuple

from ..models.types import DirEntry, Item


class StorageBackend(ABC):
    """Abstract storage backend interface."""

    @abstractmethod
    async def exists(self, path: str) -> bool:
        """Check if path exists."""
        pass

    @abstractmethod
    async def read_bytes(self, path: str) -> bytes:
        """Read file as bytes."""
        pass

    @abstractmethod
    async def read_text(self, path: str) -> str:
        """Read file as text."""
        pass

    @abstractmethod
    async def write_bytes(self, path: str, data: bytes) -> None:
        """Write bytes to file."""
        pass

    @abstractmethod
    async def write_text(self, path: str, content: str) -> None:
        """Write text to file."""
        pass

    @abstractmethod
    async def list_directory(self, path: str) -> Tuple[List[DirEntry], List[Item]]:
        """List directory contents, returns (dirs, items)."""
        pass

    @abstractmethod
    async def get_file_info(self, path: str) -> Optional[Tuple[int, int, int]]:
        """Get file info: (width, height, size) or None if not found."""
        pass

    @abstractmethod
    async def delete(self, path: str) -> None:
        """Delete file."""
        pass

    @abstractmethod
    def get_public_url(self, path: str) -> str:
        """Get public URL for file access."""
        pass

    @abstractmethod
    async def health_check(self) -> dict:
        """Return storage backend health status."""
        pass
```

## src/lenscat_backend/storage/__init__.py

- Characters: 181
- Tokens: 0

```python
"""Storage backends package."""
from .base import StorageBackend
from .local import LocalStorage
from .s3 import S3Storage

__all__ = ["StorageBackend", "LocalStorage", "S3Storage"]
```

## src/lenscat_backend/storage/s3.py

- Characters: 8038
- Tokens: 0

```python
"""S3 storage backend."""
import asyncio
from typing import List, Optional, Tuple
from urllib.parse import quote

import aioboto3
from botocore.exceptions import ClientError, NoCredentialsError
from PIL import Image
import io

from ..models.types import DirEntry, DirKind, Item, MimeType
from .base import StorageBackend


class S3Storage(StorageBackend):
    """S3 storage implementation."""

    def __init__(self, bucket: str, prefix: str = "", region: str = "us-east-1"):
        """Initialize S3 storage."""
        self.bucket = bucket
        self.prefix = prefix.rstrip("/")
        self.region = region
        self.session = aioboto3.Session()

    def _full_path(self, path: str) -> str:
        """Get full S3 key with prefix."""
        clean_path = path.lstrip("/")
        if self.prefix:
            return f"{self.prefix}/{clean_path}"
        return clean_path

    async def exists(self, path: str) -> bool:
        """Check if S3 object exists."""
        try:
            async with self.session.client("s3", region_name=self.region) as s3:
                await s3.head_object(Bucket=self.bucket, Key=self._full_path(path))
                return True
        except ClientError as e:
            if e.response["Error"]["Code"] == "404":
                return False
            raise

    async def read_bytes(self, path: str) -> bytes:
        """Read S3 object as bytes."""
        async with self.session.client("s3", region_name=self.region) as s3:
            response = await s3.get_object(Bucket=self.bucket, Key=self._full_path(path))
            return await response["Body"].read()

    async def read_text(self, path: str) -> str:
        """Read S3 object as text."""
        data = await self.read_bytes(path)
        return data.decode("utf-8")

    async def write_bytes(self, path: str, data: bytes) -> None:
        """Write bytes to S3."""
        async with self.session.client("s3", region_name=self.region) as s3:
            await s3.put_object(
                Bucket=self.bucket,
                Key=self._full_path(path),
                Body=data
            )

    async def write_text(self, path: str, content: str) -> None:
        """Write text to S3."""
        await self.write_bytes(path, content.encode("utf-8"))

    async def list_directory(self, path: str) -> Tuple[List[DirEntry], List[Item]]:
        """List S3 directory contents."""
        prefix = self._full_path(path)
        if prefix and not prefix.endswith("/"):
            prefix += "/"

        dirs = []
        items = []
        seen_dirs = set()

        async with self.session.client("s3", region_name=self.region) as s3:
            paginator = s3.get_paginator("list_objects_v2")
            
            async for page in paginator.paginate(
                Bucket=self.bucket, 
                Prefix=prefix,
                Delimiter="/"
            ):
                # Handle directories (common prefixes)
                for common_prefix in page.get("CommonPrefixes", []):
                    dir_name = common_prefix["Prefix"][len(prefix):].rstrip("/")
                    if dir_name and dir_name not in seen_dirs:
                        seen_dirs.add(dir_name)
                        # TODO: Determine if branch/leaf-real/leaf-pointer
                        dirs.append(DirEntry(name=dir_name, kind=DirKind.BRANCH))

                # Handle files
                for obj in page.get("Contents", []):
                    key = obj["Key"]
                    if key == prefix:  # Skip the directory itself
                        continue
                    
                    rel_key = key[len(prefix):]
                    if "/" in rel_key:  # Skip nested files
                        continue
                    
                    if self._is_image_file(rel_key):
                        # Get image info
                        try:
                            info = await self.get_file_info(key[len(self.prefix)+1:] if self.prefix else key)
                            if info:
                                w, h, size = info
                                
                                # Check for sidecar and thumbnail
                                has_meta = await self.exists(f"{key}.json")
                                has_thumb = await self.exists(f"{key}.thumbnail")
                                
                                items.append(Item(
                                    path=key[len(self.prefix)+1:] if self.prefix else key,
                                    name=rel_key,
                                    type=self._get_mime_type(rel_key),
                                    w=w,
                                    h=h,
                                    size=size,
                                    hasThumb=has_thumb,
                                    hasMeta=has_meta
                                ))
                        except Exception:
                            continue

        return dirs, items

    async def get_file_info(self, path: str) -> Optional[Tuple[int, int, int]]:
        """Get S3 object info: (width, height, size)."""
        try:
            if not self._is_image_file(path):
                return None
            
            # Get object metadata first for size
            async with self.session.client("s3", region_name=self.region) as s3:
                response = await s3.head_object(
                    Bucket=self.bucket,
                    Key=self._full_path(path)
                )
                size = response["ContentLength"]
                
                # Download image to get dimensions
                obj_response = await s3.get_object(
                    Bucket=self.bucket,
                    Key=self._full_path(path)
                )
                data = await obj_response["Body"].read()
                
                with Image.open(io.BytesIO(data)) as img:
                    w, h = img.size
                
                return w, h, size
        except Exception:
            return None

    async def delete(self, path: str) -> None:
        """Delete S3 object."""
        async with self.session.client("s3", region_name=self.region) as s3:
            await s3.delete_object(Bucket=self.bucket, Key=self._full_path(path))

    def get_public_url(self, path: str) -> str:
        """Get public S3 URL."""
        key = self._full_path(path)
        return f"https://{self.bucket}.s3.{self.region}.amazonaws.com/{quote(key)}"

    async def health_check(self) -> dict:
        """Return S3 health status."""
        try:
            async with self.session.client("s3", region_name=self.region) as s3:
                # Test basic access
                await s3.head_bucket(Bucket=self.bucket)
                return {
                    "type": "s3",
                    "bucket": self.bucket,
                    "prefix": self.prefix,
                    "region": self.region,
                    "accessible": True,
                }
        except NoCredentialsError:
            return {
                "type": "s3",
                "bucket": self.bucket,
                "accessible": False,
                "error": "No AWS credentials found",
            }
        except ClientError as e:
            return {
                "type": "s3",
                "bucket": self.bucket,
                "accessible": False,
                "error": str(e),
            }

    def _is_image_file(self, path: str) -> bool:
        """Check if file is a supported image."""
        return path.lower().endswith((".jpg", ".jpeg", ".png", ".webp"))

    def _get_mime_type(self, path: str) -> MimeType:
        """Get MIME type from file extension."""
        ext = path.lower().split(".")[-1]
        if ext in {"jpg", "jpeg"}:
            return MimeType.JPEG
        elif ext == "png":
            return MimeType.PNG
        elif ext == "webp":
            return MimeType.WEBP
        else:
            return MimeType.JPEG  # fallback
```

## src/lenscat_backend/storage/local.py

- Characters: 6066
- Tokens: 0

```python
"""Local filesystem storage backend."""
import os
from pathlib import Path
from typing import List, Optional, Tuple

import aiofiles
from PIL import Image

from ..models.types import DirEntry, DirKind, Item, MimeType
from .base import StorageBackend


class LocalStorage(StorageBackend):
    """Local filesystem storage implementation."""

    def __init__(self, root_path: str):
        """Initialize with root directory path."""
        self.root = Path(root_path).resolve()
        self.root.mkdir(parents=True, exist_ok=True)

    def _resolve_path(self, path: str) -> Path:
        """Resolve relative path to absolute path within root."""
        # Remove leading slash and resolve
        clean_path = path.lstrip("/")
        resolved = (self.root / clean_path).resolve()
        
        # Security check: ensure path is within root
        if not str(resolved).startswith(str(self.root)):
            raise ValueError(f"Path {path} is outside root directory")
        
        return resolved

    async def exists(self, path: str) -> bool:
        """Check if path exists."""
        try:
            return self._resolve_path(path).exists()
        except ValueError:
            return False

    async def read_bytes(self, path: str) -> bytes:
        """Read file as bytes."""
        file_path = self._resolve_path(path)
        async with aiofiles.open(file_path, "rb") as f:
            return await f.read()

    async def read_text(self, path: str) -> str:
        """Read file as text."""
        file_path = self._resolve_path(path)
        async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
            return await f.read()

    async def write_bytes(self, path: str, data: bytes) -> None:
        """Write bytes to file."""
        file_path = self._resolve_path(path)
        file_path.parent.mkdir(parents=True, exist_ok=True)
        async with aiofiles.open(file_path, "wb") as f:
            await f.write(data)

    async def write_text(self, path: str, content: str) -> None:
        """Write text to file."""
        file_path = self._resolve_path(path)
        file_path.parent.mkdir(parents=True, exist_ok=True)
        async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
            await f.write(content)

    async def list_directory(self, path: str) -> Tuple[List[DirEntry], List[Item]]:
        """List directory contents."""
        dir_path = self._resolve_path(path)
        if not dir_path.is_dir():
            return [], []

        dirs = []
        items = []
        
        for entry in dir_path.iterdir():
            if entry.is_dir():
                # Determine directory kind
                kind = DirKind.BRANCH
                if any(self._is_image_file(f) for f in entry.iterdir() if f.is_file()):
                    kind = DirKind.LEAF_REAL
                elif (entry / ".lenscat.folder.json").exists():
                    kind = DirKind.LEAF_POINTER
                
                dirs.append(DirEntry(name=entry.name, kind=kind))
            
            elif entry.is_file() and self._is_image_file(entry):
                # Get image info
                try:
                    info = await self.get_file_info(str(entry.relative_to(self.root)))
                    if info:
                        w, h, size = info
                        rel_path = str(entry.relative_to(self.root))
                        
                        # Check for sidecar and thumbnail
                        has_meta = (entry.parent / f"{entry.name}.json").exists()
                        has_thumb = (entry.parent / f"{entry.name}.thumbnail").exists()
                        
                        items.append(Item(
                            path=rel_path,
                            name=entry.name,
                            type=self._get_mime_type(entry),
                            w=w,
                            h=h,
                            size=size,
                            hasThumb=has_thumb,
                            hasMeta=has_meta
                        ))
                except Exception:
                    # Skip files we can't process
                    continue

        return dirs, items

    async def get_file_info(self, path: str) -> Optional[Tuple[int, int, int]]:
        """Get image file info: (width, height, size)."""
        try:
            file_path = self._resolve_path(path)
            if not file_path.exists() or not self._is_image_file(file_path):
                return None
            
            # Get file size
            size = file_path.stat().st_size
            
            # Get image dimensions
            with Image.open(file_path) as img:
                w, h = img.size
            
            return w, h, size
        except Exception:
            return None

    async def delete(self, path: str) -> None:
        """Delete file."""
        file_path = self._resolve_path(path)
        if file_path.exists():
            file_path.unlink()

    def get_public_url(self, path: str) -> str:
        """Get public URL for file access."""
        # For local storage, return a file:// URL or relative path
        return f"/api/files/{path.lstrip('/')}"

    async def health_check(self) -> dict:
        """Return storage health status."""
        return {
            "type": "local",
            "root_path": str(self.root),
            "writable": os.access(self.root, os.W_OK),
            "exists": self.root.exists(),
        }

    def _is_image_file(self, path: Path) -> bool:
        """Check if file is a supported image."""
        return path.suffix.lower() in {".jpg", ".jpeg", ".png", ".webp"}

    def _get_mime_type(self, path: Path) -> MimeType:
        """Get MIME type from file extension."""
        ext = path.suffix.lower()
        if ext in {".jpg", ".jpeg"}:
            return MimeType.JPEG
        elif ext == ".png":
            return MimeType.PNG
        elif ext == ".webp":
            return MimeType.WEBP
        else:
            return MimeType.JPEG  # fallback
```

## pyproject.toml

- Characters: 361
- Tokens: 0

```text
[project]
name = "lenscat-backend"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
authors = [
    { name = "yada", email = "trojblue@gmail.com" }
]
requires-python = ">=3.10"
dependencies = []

[project.scripts]
lenscat-backend = "lenscat_backend:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

## data/events/2024/conference/keynote.jpg.json

- Characters: 286
- Tokens: 0

```json
{
  "v": 1,
  "tags": [
    "conference",
    "keynote",
    "2024"
  ],
  "notes": "Opening keynote",
  "exif": {
    "width": 400,
    "height": 300,
    "created_at": "2024-01-01T12:00:00Z"
  },
  "updated_at": "2025-09-09T21:31:47.105925Z",
  "updated_by": "sample-data-generator"
}
```

## data/events/2024/conference/panel.jpg.json

- Characters: 287
- Tokens: 0

```json
{
  "v": 1,
  "tags": [
    "conference",
    "panel",
    "discussion"
  ],
  "notes": "Expert panel",
  "exif": {
    "width": 400,
    "height": 300,
    "created_at": "2024-01-01T12:00:00Z"
  },
  "updated_at": "2025-09-09T21:31:47.107058Z",
  "updated_by": "sample-data-generator"
}
```

## data/portraits/person1.jpg.json

- Characters: 283
- Tokens: 0

```json
{
  "v": 1,
  "tags": [
    "portrait",
    "professional"
  ],
  "notes": "Professional headshot",
  "exif": {
    "width": 400,
    "height": 300,
    "created_at": "2024-01-01T12:00:00Z"
  },
  "updated_at": "2025-09-09T21:31:47.101488Z",
  "updated_by": "sample-data-generator"
}
```

## data/portraits/person2.jpg.json

- Characters: 271
- Tokens: 0

```json
{
  "v": 1,
  "tags": [
    "portrait",
    "casual"
  ],
  "notes": "Casual portrait",
  "exif": {
    "width": 400,
    "height": 300,
    "created_at": "2024-01-01T12:00:00Z"
  },
  "updated_at": "2025-09-09T21:31:47.102499Z",
  "updated_by": "sample-data-generator"
}
```

## data/landscapes/sunset.jpg.json

- Characters: 288
- Tokens: 0

```json
{
  "v": 1,
  "tags": [
    "landscape",
    "sunset",
    "golden"
  ],
  "notes": "Golden hour magic",
  "exif": {
    "width": 400,
    "height": 300,
    "created_at": "2024-01-01T12:00:00Z"
  },
  "updated_at": "2025-09-09T21:31:47.100241Z",
  "updated_by": "sample-data-generator"
}
```

## data/landscapes/mountain.jpg.json

- Characters: 296
- Tokens: 0

```json
{
  "v": 1,
  "tags": [
    "landscape",
    "mountain",
    "nature"
  ],
  "notes": "Beautiful mountain view",
  "exif": {
    "width": 400,
    "height": 300,
    "created_at": "2024-01-01T12:00:00Z"
  },
  "updated_at": "2025-09-09T21:31:47.097760Z",
  "updated_by": "sample-data-generator"
}
```

## data/landscapes/lake.jpg.json

- Characters: 289
- Tokens: 0

```json
{
  "v": 1,
  "tags": [
    "landscape",
    "water",
    "serene"
  ],
  "notes": "Peaceful lake scene",
  "exif": {
    "width": 400,
    "height": 300,
    "created_at": "2024-01-01T12:00:00Z"
  },
  "updated_at": "2025-09-09T21:31:47.099097Z",
  "updated_by": "sample-data-generator"
}
```

## data/architecture/bridge.jpg.json

- Characters: 284
- Tokens: 0

```json
{
  "v": 1,
  "tags": [
    "architecture",
    "bridge",
    "urban"
  ],
  "notes": "City bridge",
  "exif": {
    "width": 400,
    "height": 300,
    "created_at": "2024-01-01T12:00:00Z"
  },
  "updated_at": "2025-09-09T21:31:47.104699Z",
  "updated_by": "sample-data-generator"
}
```

## data/architecture/building1.jpg.json

- Characters: 282
- Tokens: 0

```json
{
  "v": 1,
  "tags": [
    "architecture",
    "modern"
  ],
  "notes": "Modern building design",
  "exif": {
    "width": 400,
    "height": 300,
    "created_at": "2024-01-01T12:00:00Z"
  },
  "updated_at": "2025-09-09T21:31:47.103629Z",
  "updated_by": "sample-data-generator"
}
```

## requirements.txt

- Characters: 143
- Tokens: 0

```text
# Alternative to pyproject.toml for simpler deployment
fastapi
uvicorn[standard]
aioboto3
pyvips
pillow
orjson
blake3
aiofiles
python-multipart
```

## env.example

- Characters: 371
- Tokens: 0

```text
# Storage Configuration
STORAGE_TYPE=local  # or s3
LOCAL_ROOT=./data

# S3 Configuration (if STORAGE_TYPE=s3)
S3_BUCKET=my-gallery-bucket
S3_PREFIX=gallery/
S3_REGION=us-east-1

# Server Configuration
HOST=0.0.0.0
PORT=8000
RELOAD=true

# AWS Credentials (for S3 storage)
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# AWS_REGION=us-east-1
```

## README.md

- Characters: 4327
- Tokens: 0

````markdown
# Lenscat Backend

Minimal FastAPI backend for the Lenscat gallery system. Follows the "boring, fast, minimal" philosophy with flat file storage and async workers.

## Features

- **Flat file storage**: No database required, uses JSON manifests and sidecars
- **Dual storage**: Local filesystem or S3 with same API
- **Fast thumbnails**: pyvips (with PIL fallback) for WebP generation
- **Smart indexing**: On-demand folder manifest building
- **Simple search**: Full-text search across filenames, tags, and notes
- **Performance-first**: Async workers, caching, minimal dependencies

## Quick Start

### 1. Install Dependencies

```bash
# Using pip
pip install -r requirements.txt

# Or using poetry
poetry install
```

### 2. Create Sample Data (Optional)

```bash
python scripts/create_sample_data.py
```

### 3. Run Development Server

```bash
python scripts/dev.py
```

The server will start at `http://localhost:8000` with API docs at `/docs`.

## Configuration

Copy `env.example` to `.env` and configure:

```bash
# Storage type
STORAGE_TYPE=local  # or s3

# Local storage
LOCAL_ROOT=./data

# S3 storage (if STORAGE_TYPE=s3)
S3_BUCKET=my-gallery-bucket
S3_PREFIX=gallery/
S3_REGION=us-east-1

# Server
HOST=0.0.0.0
PORT=8000
```

## API Endpoints

### Core Endpoints

- `GET /api/folders?path=<path>` - List folder contents
- `GET /api/item?path=<path>` - Get item metadata
- `PUT /api/item?path=<path>` - Update item metadata
- `GET /api/thumb?path=<path>` - Get/generate thumbnail
- `GET /api/search?q=<query>` - Search items
- `GET /api/health` - System health check

### File Structure

The backend expects this file organization:

```
data/
â”œâ”€â”€ _index.json           # Root folder manifest
â”œâ”€â”€ _rollup.json         # Search index
â”œâ”€â”€ image1.jpg           # Image file
â”œâ”€â”€ image1.jpg.json      # Sidecar metadata
â”œâ”€â”€ image1.jpg.thumbnail # WebP thumbnail
â””â”€â”€ subfolder/
    â”œâ”€â”€ _index.json      # Subfolder manifest
    â””â”€â”€ ...
```

### Sidecar Format

```json
{
  "v": 1,
  "tags": ["portrait", "professional"],
  "notes": "Great headshot for website",
  "exif": {
    "width": 1200,
    "height": 900,
    "created_at": "2024-01-15T10:30:00Z"
  },
  "hash": "blake3:abc123...",
  "updated_at": "2024-01-15T15:45:00Z",
  "updated_by": "user@device"
}
```

## Architecture

### Storage Backends

- **LocalStorage**: Direct filesystem access
- **S3Storage**: AWS S3 with aioboto3

### Workers

- **FolderIndexer**: Builds `_index.json` manifests
- **ThumbnailWorker**: Generates missing thumbnails  
- **RollupBuilder**: Creates search index `_rollup.json`

### Performance Features

- Lazy manifest building (only when needed)
- Concurrent thumbnail generation
- BLAKE3 hashing (fallback to SHA256)
- WebP thumbnails with quality optimization
- HTTP caching headers

## Development

### Project Structure

```
src/
â”œâ”€â”€ main.py              # FastAPI app
â”œâ”€â”€ api/                 # API endpoints
â”œâ”€â”€ models/              # Pydantic models
â”œâ”€â”€ storage/             # Storage backends
â”œâ”€â”€ workers/             # Background workers
â””â”€â”€ utils/               # Utilities (thumbnails, EXIF, hashing)
```

### Adding Storage Backends

1. Inherit from `StorageBackend`
2. Implement all abstract methods
3. Add to `dependencies.py`

### Performance Guidelines

- Keep functions small (< 50 lines)
- Use async/await everywhere
- Batch operations when possible
- Cache aggressively but invalidate correctly
- Prefer orjson over json for performance

## Deployment

### Docker (Recommended)

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
EXPOSE 8000

CMD ["python", "-m", "src.main"]
```

### Environment Variables

```bash
STORAGE_TYPE=s3
S3_BUCKET=production-gallery
S3_PREFIX=images/
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
```

## Monitoring

- `GET /api/health` - Overall system health
- `GET /api/health/storage` - Storage backend status  
- `GET /api/health/workers` - Worker statistics

## Troubleshooting

### Common Issues

1. **No thumbnails showing**: Check pyvips installation
2. **S3 access denied**: Verify AWS credentials and bucket permissions
3. **Slow indexing**: Consider pagination for large folders

### Debug Mode

Set `RELOAD=true` for auto-reload during development.

## License

MIT License - see LICENSE file.
````

## scripts/dev.py

- Characters: 1216
- Tokens: 0

```python
#!/usr/bin/env python3
"""Development server runner."""
import os
import sys
from pathlib import Path

# Add src to path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

if __name__ == "__main__":
    import uvicorn
    
    # Set development environment
    os.environ.setdefault("STORAGE_TYPE", "local")
    os.environ.setdefault("LOCAL_ROOT", "./data")
    os.environ.setdefault("HOST", "0.0.0.0")
    os.environ.setdefault("PORT", "8000")
    os.environ.setdefault("RELOAD", "true")
    
    # Create data directory if it doesn't exist
    data_dir = Path("./data")
    data_dir.mkdir(exist_ok=True)
    
    # Add some sample images if directory is empty
    if not any(data_dir.iterdir()):
        print("ðŸ“ Data directory is empty. Add some images to ./data to see them in the gallery.")
    
    print("ðŸš€ Starting Lenscat backend in development mode...")
    print(f"ðŸ“‚ Storage: Local ({data_dir.resolve()})")
    print(f"ðŸŒ Server: http://localhost:8000")
    print(f"ðŸ“– API Docs: http://localhost:8000/docs")
    
    uvicorn.run(
        "main:app",
        host=os.environ["HOST"],
        port=int(os.environ["PORT"]),
        reload=True,
        reload_dirs=["src"]
    )
```

## scripts/create_sample_data.py

- Characters: 3767
- Tokens: 0

```python
#!/usr/bin/env python3
"""Create sample data for development."""
import json
import os
from datetime import datetime
from pathlib import Path
from PIL import Image, ImageDraw


def create_sample_image(path: Path, width: int = 400, height: int = 300, color: str = "blue"):
    """Create a sample image."""
    img = Image.new("RGB", (width, height), color)
    draw = ImageDraw.Draw(img)
    
    # Add some text
    text = path.stem
    draw.text((10, 10), text, fill="white")
    
    # Save as JPEG
    img.save(path, "JPEG", quality=85)


def create_sample_sidecar(image_path: Path, tags: list, notes: str = ""):
    """Create a sample sidecar file."""
    sidecar_path = Path(f"{image_path}.json")
    
    sidecar_data = {
        "v": 1,
        "tags": tags,
        "notes": notes,
        "exif": {
            "width": 400,
            "height": 300,
            "created_at": "2024-01-01T12:00:00Z"
        },
        "updated_at": datetime.utcnow().isoformat() + "Z",
        "updated_by": "sample-data-generator"
    }
    
    with open(sidecar_path, "w") as f:
        json.dump(sidecar_data, f, indent=2)


def main():
    """Create sample data structure."""
    data_dir = Path("./data")
    data_dir.mkdir(exist_ok=True)
    
    # Create folder structure
    folders = {
        "landscapes": [
            ("mountain.jpg", ["landscape", "mountain", "nature"], "Beautiful mountain view"),
            ("lake.jpg", ["landscape", "water", "serene"], "Peaceful lake scene"),
            ("sunset.jpg", ["landscape", "sunset", "golden"], "Golden hour magic"),
        ],
        "portraits": [
            ("person1.jpg", ["portrait", "professional"], "Professional headshot"),
            ("person2.jpg", ["portrait", "casual"], "Casual portrait"),
        ],
        "architecture": [
            ("building1.jpg", ["architecture", "modern"], "Modern building design"),
            ("bridge.jpg", ["architecture", "bridge", "urban"], "City bridge"),
        ]
    }
    
    # Create sample images and metadata
    for folder_name, images in folders.items():
        folder_path = data_dir / folder_name
        folder_path.mkdir(exist_ok=True)
        
        for image_name, tags, notes in images:
            image_path = folder_path / image_name
            
            if not image_path.exists():
                # Create sample image with different colors
                color = "blue" if "landscape" in tags else "green" if "portrait" in tags else "red"
                create_sample_image(image_path, color=color)
                
                # Create sidecar
                create_sample_sidecar(image_path, tags, notes)
                
                print(f"Created: {image_path}")
    
    # Create a nested structure
    nested_dir = data_dir / "events" / "2024" / "conference"
    nested_dir.mkdir(parents=True, exist_ok=True)
    
    conference_images = [
        ("keynote.jpg", ["conference", "keynote", "2024"], "Opening keynote"),
        ("panel.jpg", ["conference", "panel", "discussion"], "Expert panel"),
    ]
    
    for image_name, tags, notes in conference_images:
        image_path = nested_dir / image_name
        if not image_path.exists():
            create_sample_image(image_path, color="purple")
            create_sample_sidecar(image_path, tags, notes)
            print(f"Created: {image_path}")
    
    print(f"\nâœ… Sample data created in {data_dir.resolve()}")
    print("ðŸ–¼ï¸  Total structure:")
    print("   ./data/landscapes/ (3 images)")
    print("   ./data/portraits/ (2 images)")  
    print("   ./data/architecture/ (2 images)")
    print("   ./data/events/2024/conference/ (2 images)")
    print("\nðŸš€ Start the backend with: python scripts/dev.py")


if __name__ == "__main__":
    main()
```

## Statistics

- Total Files: 37
- Total Characters: 73872
- Total Tokens: 0
